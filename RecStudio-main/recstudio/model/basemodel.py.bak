import abc
import os
import inspect
from typing import Dict, List, Optional, Tuple, Union

import copy
import time
# import nni
import recstudio.eval as eval
import torch
import torch.optim as optim
from recstudio.ann import sampler
from recstudio.data.advance_dataset import ALSDataset, SessionDataset
from recstudio.data.dataset import (AEDataset, FullSeqDataset, MFDataset,
                                    SeqDataset, CombinedLoaders)
from recstudio.model import init, loss_func
from recstudio.model import scorer
from recstudio.utils.utils import color_dict, print_logger, seed_everything


class Recommender(torch.nn.Module, abc.ABC):
    def __init__(self, config: Dict):
        super(Recommender, self).__init__()
        seed_everything(config['seed'], workers=True)
        self.config = config
        self.loss_fn = self._get_loss_func()
        self.fields = self.config['use_fields']
        self.embed_dim = self.config['embed_dim']

        self.logged_metrics = {}

        pass


    def fit(
        self, 
        train_data: MFDataset, 
        val_data: Optional[MFDataset]=None, 
        run_mode='light'
        ) -> None:
        r"""
        Fit the model with train data.
        """
        self.run_mode = run_mode
        self.val_check = val_data is not None and self.config['val_metrics'] is not None
        
        if self.val_check:
            self.val_metric = next(iter(self.config['val_metrics'])) \
                if isinstance(self.config['val_metrics'], list) \
                else self.config['val_metrics']
            cutoffs = self.config['cutoff'] \
                if isinstance(self.config['cutoff'], list) \
                else [self.config['cutoff']]
            if len(eval.get_rank_metrics(self.val_metric)) > 0:
                self.val_metric += '@' + str(cutoffs[0])
        # logger = TensorBoardLogger(save_dir=save_dir, name="tensorboard")

        if run_mode == 'tune' and "NNI_OUTPUT_DIR" in os.environ: 
            save_dir = os.environ["NNI_OUTPUT_DIR"] #for parameter tunning
        else:
            save_dir = os.getcwd()
        print_logger.info('save_dir:' + save_dir)
        # refresh_rate = 0 if run_mode in ['light', 'tune'] else 1


        self.trainloaders = self._get_train_loaders(train_data)

        self._init_model(train_data)
        self._init_parameter()

        self.optimizers = self._get_optimizers()

        if val_data:
            val_loader = val_data.eval_loader(
                batch_size = self.config['eval_batch_size'],
                num_workers = self.config['num_workers'])
        else:
            val_loader = None
        
        self.fit_loop(val_loader)

    
    def evaluate(self, test_data, verbose=True) -> Dict:
        r""" Predict for test data.

        Args: 
            test_data(recstudio.data.Dataset): The dataset of test data, which is generated by RecStudio.

            verbose(bool, optimal): whether to show the detailed information.

        Returns:
            dict: dict of metrics. The key is the name of metrics.
        """
        test_data.drop_feat(self.fields)
        test_loader = test_data.eval_loader(batch_size=self.config['eval_batch_size'], \
            num_workers=self.config['num_workers'])
        output = {}
        self.load_checkpoint(self._best_ckpt_path)
        output_list = self.test_epoch(test_loader)
        output.update( self.test_epoch_end(output_list) )
        if self.run_mode == 'tune':
            output['default'] = output[self.val_metric]
            # nni.report_final_result(output)
        if verbose:
            print_logger.info(color_dict(output, self.run_mode=='tune'))
        return output


    def predict(self, batch, k, *args):
        pass


    @abc.abstractmethod
    def forward(self, batch):
        pass


    def load_dataset(self, data_config_file: str):
        r"""Load dataset for the model.

        Args:
            data_config_file(str): the file path of the dataset file.

        Returns:
            list: A list contains train/valid/test data-[train, valid, test]
        """
        cls = self._get_dataset_class()
        dataset = cls(data_config_file)
        if cls in (MFDataset, ALSDataset):
            parameter = {'shuffle': self.config.get('shuffle'),
                        'split_mode': self.config.get('split_mode')}
            if isinstance(self, BaseRanker):
                parameter['fmeval'] = True
        elif cls == AEDataset:
            parameter = {'shuffle': self.config.get('shuffle')}
        elif cls in (SeqDataset, FullSeqDataset):
            parameter = {'rep' : self.config.get('test_repetitive'),
                         'train_rep': self.config.get('train_repetitive')}
        if cls != ALSDataset:
            parameter.update({
                'dataset_sampling_count': self.config.get('dataset_sampling_count'),
                'dataset_sampler': self.config.get('dataset_sampler')
            })
        parameter = {k: v for k, v in parameter.items() if v is not None}
        return dataset.build(self.config['split_ratio'], **parameter)


    def current_epoch_trainloaders(self, nepoch) -> List:
        # use nepoch to config current trainloaders
        # TODO: combine multi loaders. 
        combine = False
        return self.trainloaders, combine


    def current_epoch_optimizers(self, nepoch) -> List:
        # use nepoch to config current optimizers
        return self.optimizers


    @abc.abstractmethod
    def sampling(self):
        pass

    @abc.abstractmethod
    def build_index(self):
        pass


    @abc.abstractmethod
    def training_step(self, batch):
        pass 


    @abc.abstractmethod
    def validation_step(self, batch):
        pass


    @abc.abstractmethod
    def test_step(self, batch):
        pass


    def training_epoch_end(self, outputs):
        if isinstance(outputs, List):
            loss_metric = {'train_'+ k: torch.hstack([e[k] for e in outputs]).mean() for k in outputs[0]}
        elif isinstance(outputs, torch.Tensor):
            loss_metric = {'train_loss': outputs.item()}
        elif isinstance(outputs, Dict):
            loss_metric = {'train_'+k : v for k, v in outputs}
        
        self.log_dict(loss_metric)
        if self.val_check and self.run_mode == 'tune':
            metric = self.logged_metrics[self.val_metric]
            # nni.report_intermediate_result(metric)
        if self.run_mode in ['light', 'tune'] or self.val_check:
            print_logger.info(color_dict(self.logged_metrics, self.run_mode=='tune'))
        else:
            print_logger.info('\n'+color_dict(self.logged_metrics, self.run_mode=='tune'))
            
    
    def validation_epoch_end(self, outputs):
        val_metric = self.config['val_metrics'] if isinstance(self.config['val_metrics'], list) else [self.config['val_metrics']]
        cutoffs = self.config['cutoff'] if isinstance(self.config['cutoff'], list) else [self.config.setdefault('cutoff', None)]
        val_metric = [f'{m}@{cutoff}' if len(eval.get_rank_metrics(m)) > 0 else m  for cutoff in cutoffs[:1] for m in val_metric]
        out = self._test_epoch_end(outputs)
        out = dict(zip(val_metric, out))
        self.log_dict(out)
        return out


    def test_epoch_end(self, outputs):
        test_metric = self.config['test_metrics'] if isinstance(self.config['test_metrics'], list) else [self.config['test_metrics']]
        cutoffs = self.config['cutoff'] if isinstance(self.config['cutoff'], list) else [self.config.setdefault('cutoff', None)]
        test_metric = [f'{m}@{cutoff}' if len(eval.get_rank_metrics(m)) > 0 else m for cutoff in cutoffs for m in test_metric]
        out = self._test_epoch_end(outputs)
        out = dict(zip(test_metric, out))
        self.log_dict(out)
        return out


    def _test_epoch_end(self, outputs):
        metric, bs = zip(*outputs)
        metric = torch.tensor(metric)
        bs = torch.tensor(bs)
        out = (metric * bs.view(-1, 1)).sum(0) / bs.sum()
        return out


    def log_dict(self, metrics: Dict):
        self.logged_metrics.update(metrics)


    def _init_model(self, train_data, retriever=None):
        """Init model with some necessary layers.

        Model structures are expected to be config here. Here is a simple example for GRU4Rec model:

        **Example**

            >>> self.hidden_size = self.config['hidden_size']
            >>> self.num_layers = self.config['layer_num']
            >>> self.dropout_rate = self.config['dropout_rate']
            >>> self.emb_dropout = torch.nn.Dropout(self.dropout_rate)
            >>> self.GRU = torch.nn.GRU(
            >>>     input_size = self.embed_dim,
            >>>     hidden_size = self.hidden_size,
            >>>     num_layers = self.num_layers,
            >>>     bias = False,
            >>>     batch_first = True,
            >>>     bidirectional = False
            >>> )
            >>> self.dense = torch.nn.Linear(self.hidden_size, self.embed_dim)

        Args:
            train_data(recstudio.data.Dataset): the train dataset. 
        """
        self.retriever = retriever
        self.frating = train_data.frating
        if self.fields is not None:
            assert self.frating in self.fields
            train_data.drop_feat(self.fields)
        else:
            self.fields = set(f for f in train_data.field2type if 'time' not in f)
            
        self.user_fields = set(train_data.user_feat.fields).intersection(self.fields)
        self.item_fields = set(train_data.item_feat.fields).intersection(self.fields)
        self.config_sampler(train_data)


    def _init_parameter(self):
        # TODO: init parameter will init retriever and sampler
        for name, module in self.named_children():
            if name not in ['sampler', 'retriever']:
                if isinstance(module, Recommender):
                    module._init_parameter()
                else:
                    module.apply(init.xavier_normal_initialization)


    @abc.abstractmethod
    def _get_dataset_class(self):
        pass


    @abc.abstractmethod
    def _get_loss_func(self):
        pass

    
    def _get_item_feat(self, data):
        if isinstance(data, dict): ## batch_data
            if len(self.item_fields) == 1:
                return data[self.fiid]
            else:
                return dict((field, value) for field, value in data.items() if field in self.item_fields)
        else: ## neg_item_idx
            if len(self.item_fields) == 1:
                return data
            else:
                return self.item_feat[data]


    def _get_train_loaders(self, train_data) -> List:
        # TODO: modify loaders in model
        # train_data.loaders = [train_data.loader]
        # train_data.nepoch = None
        return [train_data.loader(
            batch_size = self.config['batch_size'],
            shuffle = False, 
            num_workers = self.config['num_workers'], 
            drop_last = False)]


    def _get_optimizers(self, optimizer_config: Dict=None) -> List[Dict]:
        # TODO: multi optimizers
        if isinstance(self.config['learner'], list):
            print_logger.warning("If you want to use multi learner, please \
                override `_get_optimizers` function. We will use the first \
                learner for all the parameters.")
            opt_name = self.config['learner'][0]
            lr = self.config['learning_rate'][0]
            weight_decay = None if self.config['weight_decay'] is None \
                else self.config['weight_decay'][0] 
            scheduler_name = None if self.config['scheduler'] is None \
                else self.config['scheduler'][0] 
        else:
            opt_name = self.config['learner']
            lr = self.config['learning_rate']
            weight_decay = self.config['weight_decay']
            scheduler_name = self.config['scheduler']
        params = self.parameters()
        optimizer = self._get_optimizer(opt_name, params, lr, weight_decay)
        scheduler = self._get_scheduler(scheduler_name, optimizer)
        m = self.val_metric if self.val_check else 'train_loss'
        if scheduler:
            return [{
                'optimizer': optimizer,
                'lr_scheduler': {
                    'scheduler': scheduler,
                    'monitor': m,
                    'interval': 'epoch',
                    'frequency': 1,
                    'strict': False
                }
            }]
        else:
            return [{'optimizer': optimizer}]

    
    def _get_optimizer(self, name, params, lr, weight_decay):
        r"""Return optimizer for specific parameters.

        The optimizer can be configured in the config file with the key ``learner``. 
        Supported optimizer: ``Adam``, ``SGD``, ``AdaGrad``, ``RMSprop``, ``SparseAdam``.

        .. note::
            If no learner is assigned in the configuration file, then ``Adam`` will be user.

        Args:
            params: the parameters to be optimized.
        
        Returns:
            torch.optim.optimizer: optimizer according to the config.
        """
        # '''@nni.variable(nni.choice(0.1, 0.05, 0.01, 0.005, 0.001), name=learning_rate)'''
        learning_rate = lr
        # '''@nni.variable(nni.choice(0.1, 0.01, 0.001, 0), name=decay)'''
        decay = weight_decay
        if name.lower() == 'adam':
            optimizer = optim.Adam(params, lr=learning_rate, weight_decay=decay)
        elif name.lower() == 'sgd':
            optimizer = optim.SGD(params, lr=learning_rate, weight_decay=decay)
        elif name.lower() == 'adagrad':
            optimizer = optim.Adagrad(params, lr=learning_rate, weight_decay=decay)
        elif name.lower() == 'rmsprop':
            optimizer = optim.RMSprop(params, lr=learning_rate, weight_decay=decay)
        elif name.lower() == 'sparse_adam':
            optimizer = optim.SparseAdam(params, lr=learning_rate)
            #if self.weight_decay > 0:
            #    self.logger.warning('Sparse Adam cannot argument received argument [{weight_decay}]')
        else:
            optimizer = optim.Adam(params, lr=learning_rate)
        return optimizer


    def _get_scheduler(self, name, optimizer):
        r"""Return learning rate scheduler for the optimizer.

        Args:
            optimizer(torch.optim.Optimizer): the optimizer which need a scheduler.

        Returns:
            torch.optim.lr_scheduler: the learning rate scheduler.
        """
        # TODO: multi schedulers
        if name is not None:
            if name.lower() == 'exponential':
                scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.98)
            elif name.lower() == 'onplateau':
                scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)
            else:
                scheduler = None
        else:
            scheduler = None
        return scheduler


    def _get_sampler(self, train_data):
        if 'sampler' in self.config and self.config['sampler'] is not None:
            if self.config['sampler'].lower() == 'uniform':
                output = sampler.UniformSampler(train_data.num_items-1, self.score_func)
            elif self.config['sampler'].lower() == 'masked_uniform':
                output = sampler.MaskedUniformSampler(train_data.num_items-1, self.score_func)
            elif self.config['sampler'].lower() == 'popularity':
                output = sampler.PopularSamplerModel(train_data.item_freq[1:], \
                    self.score_func, self.config['item_freq_process_mode'])
            elif self.config['sampler'].lower() == 'midx_uni':
                output = sampler.MIDXSamplerUniform(train_data.num_items-1, \
                    self.config['sampler_num_centers'], self.score_func)
            elif self.config['sampler'].lower() == 'midx_pop':
                output = sampler.MIDXSamplerPop(train_data.item_freq[1:], self.config['sampler_num_centers'], \
                    self.score_func, self.config['item_freq_process_mode'])
            elif self.config['sampler'].lower() == 'cluster_uni':
                output = sampler.ClusterSamplerUniform(self.config['sampler_num_centers'], \
                    self.score_func, self.config['item_freq_process_mode'])
            elif self.config['sampler'].lower() == 'cluster_pop':
                output = sampler.ClusterSamplerPop(train_data.item_freq[1:], self.config['sampler_num_centers'], \
                    self.score_func, self.config['item_freq_process_mode'])
        else:
            output = sampler.UniformSampler(train_data.num_items-1, self.score_func)
        return output


    def config_sampler(
        self,
        train_data = None):
        if self.config['sampler'] is not None:
            if self.config['sampler'].startswith("retriever"):
                if self.retriever is None:
                    raise ValueError("retriever can not be None when it's used as sampler.")
                else:
                    method = self.config['sampler'].split('_')[1]
                    self.sampler = sampler.RetriverSampler(self.retriever, method)
            else:
                self.sampler = self._get_sampler(train_data)
        else:
            self.sampler = None


    def fit_loop(self, val_dataloader):
        try:
            nepoch = 0
            best_ckpt = {
                'config': self.config,
                'model': self.__class__.__name__,
                'epoch': 0,
                'parameters': self.state_dict(),
            }
            # set gpu
            if self.config['gpu'] is not None:
                gpus = [str(i) for i in self.config['gpu']]
                os.environ['CUDA_VISIBLE_DEVICES'] = " ".join(gpus)
                self = self.cuda()
                
            if self.val_check:
                # validation_output_list = self.validation_epoch(val_dataloader)
                # self.validation_epoch_end(validation_output_list)
                min = self.config['early_stop_mode'] == 'min' # for some metrics, smaller means better
                best_ckpt.update({
                    'metric': torch.inf if min else -torch.inf
                })
                early_stop_track = 0

            for e in range(self.config['epochs']):
                # training procedure
                tik = time.time()
                training_output_list = self.training_epoch(nepoch)
                tok_train = time.time()
                self.logged_metrics['epoch'] = nepoch
                # validation procedure
                if self.val_check:
                    validation_output_list = self.validation_epoch(val_dataloader)
                    self.validation_epoch_end(validation_output_list)
                
                tok_valid = time.time()
                self.training_epoch_end(training_output_list)
                print_logger.info("Train time: {:.5f}s. Valid time: {:.5f}s".format(
                    (tok_train-tik), (tok_valid-tik)
                ))

                # learning rate scheduler step
                optimizers = self.current_epoch_optimizers(e)
                if optimizers is not None:
                    for opt in optimizers:
                        if 'scheduler' in opt:
                            opt['scheduler'].step()

                if self.val_check:
                    current_val_metric_value = self.logged_metrics[self.val_metric].item()
                    if min:
                        if current_val_metric_value <= best_ckpt['metric']:
                            early_stop_track = 0
                            best_ckpt['metric'] = current_val_metric_value
                            best_ckpt['parameters'] = copy.deepcopy(self.state_dict())
                            best_ckpt['epoch'] = nepoch
                            print_logger.info("{} improved. Best value: {:.4f}".format(
                                self.val_metric, current_val_metric_value))
                        else:
                            early_stop_track += 1
                    else:
                        if current_val_metric_value >= best_ckpt['metric']:
                            early_stop_track = 0
                            best_ckpt['metric'] = current_val_metric_value
                            best_ckpt['parameters'] = self.state_dict()
                            best_ckpt['epoch'] = nepoch
                            print_logger.info("{} improved. Best value: {:.4f}".format(
                                self.val_metric, current_val_metric_value))
                        else:
                            early_stop_track += 1
                else:
                    best_ckpt['epoch'] = nepoch
                    best_ckpt['parameters'] = self.state_dict()


                # early stop procedure
                if early_stop_track >= self.config['early_stop_patience']:
                    print_logger.info("Early stopped. {} didn't improve for {} epochs.".format(
                            self.val_metric, early_stop_track
                        ))
                    print_logger.info("The best score of {} is {:.4f} at {}".format(
                            self.val_metric, best_ckpt['metric'], best_ckpt['epoch']
                        ))
                    break
                
                nepoch += 1
                
            # save best model
            self._best_ckpt_path = self.save_checkpoint(best_ckpt)

        except KeyboardInterrupt:
            # if catch keyboardinterrupt in training, save the best model.
            self._best_ckpt_path = self.save_checkpoint(best_ckpt)


    def training_epoch(self, nepoch):
        self.train()
        output_list = []
        optimizers = self.current_epoch_optimizers(nepoch)

        trn_dataloaders, combine = self.current_epoch_trainloaders(nepoch)
        if isinstance(trn_dataloaders, list) or isinstance(trn_dataloaders, Tuple):
            if combine:
                trn_dataloaders = [CombinedLoaders(list(trn_dataloaders))]
        else:
            trn_dataloaders = [trn_dataloaders]

        for loader in trn_dataloaders:
            for optimizer_idx, opt in enumerate(optimizers):
                for batch in loader:
                    # data to device
                    batch = self._batchdata_to_device(batch, self.config['gpu'] is not None)


                    opt['optimizer'].zero_grad()
                    # model loss
                    if 'nepoch' in inspect.getargspec(self.training_step).args:
                        loss = self.training_step(batch, nepoch)
                    else:
                        loss = self.training_step(batch)

                    if isinstance(loss, dict):
                        loss['loss'].backward()
                        output_list.append(loss)
                    elif isinstance(loss, torch.Tensor):
                        loss.backward()
                        output_list.append({"loss": loss.detach()})

                    opt['optimizer'].step()
                    

        return output_list


    def validation_epoch(self, dataloader):
        self.eval()
        if hasattr(self, '_update_item_vector'): # TODO: config frequency
            self._update_item_vector()
        output_list = []

        for batch in dataloader:
            # data to device
            batch = self._batchdata_to_device(batch, self.config['gpu'] is not None)

            # model validation results
            output = self.validation_step(batch)
            
            output_list.append(output)
        
        return output_list


    def test_epoch(self, dataloader):
        self.eval()
        if hasattr(self, '_update_item_vector'):
            self._update_item_vector()

        output_list = []

        for batch in dataloader:
            # data to device
            batch = self._batchdata_to_device(batch, self.config['gpu'] is not None)

            # model validation results
            output = self.test_step(batch)
            
            output_list.append(output)
        
        return output_list


    @staticmethod
    def _batchdata_to_device(batch, gpu=True):
        if isinstance(batch, torch.Tensor):
            return batch.cuda() if gpu else batch.cpu()
        elif isinstance(batch, Dict):
            for k in batch:
                batch[k] = Recommender._batchdata_to_device(batch[k], gpu)
            return batch
        elif isinstance(batch, List) or isinstance(batch, Tuple):
            output = []
            for b in batch:
                output.append(Recommender._batchdata_to_device(b, gpu))
            return output if isinstance(batch, List) else tuple(output)
        else:
            raise TypeError(f"`batch` is expected to be torch.Tensor, Dict, \
                List or Tuple, but {type(batch)} given.")


    def save_checkpoint(self, ckpt: Dict) -> str:
        save_path = os.path.join(self.config['save_path'])
        if not os.path.exists(save_path):
            os.makedirs(save_path)
        ckpt_name = "best_model.ckpt" #TODO: config checkpoint file name
        best_ckpt_path = os.path.join(save_path, ckpt_name)
        torch.save(ckpt, best_ckpt_path)
        print_logger.info("Best model checkpoiny saved in {}.".format(
            best_ckpt_path
        ))
        return best_ckpt_path


    def load_checkpoint(self, path: str) -> None:
        ckpt = torch.load(path)
        self.config = ckpt['config']
        self.load_state_dict(ckpt['parameters'])
            

    def sampling(
        self, 
        batch, 
        neg = 1, 
        excluding_hist: bool = False, 
        ):
        if hasattr(self, 'query_encoder'):
            # Retriever
            query = self.query_encoder(batch)
        else:
            # Ranker
            query = torch.zero(
                (batch[self.frating].size(0), 1), 
                device=batch[self.frating].device
            )
            assert isinstance(self.sampler, sampler.RetriverSampler) \
                or isinstance(self.sampler, sampler.UniformSampler) \
                or isinstance(self.sampelr, sampler.MaskedUniformSampler),\
                "sampler for ranker must be retriever or uniform sampler."

        if excluding_hist:
            if not "user_hist" in batch:
                print_logger.warning("`user_hist` are not in batch data, so the \
                    target item will be used as user_hist.")
                user_hist = batch['user_hist']
            else:
                #TODO: user hist v.s. pos item
                user_hist = batch[self.fiid]
        else:
            user_hist = None

        if isinstance(self.sampler, sampler.Sampler):
            #TODO: mask user_hist for sampler.Sampler
            kwargs = {
                'num_neg': neg, 
                'pos_items': batch[self.fiid], 
            }
            if isinstance(self.sampler, sampler.RetriverSampler):
                kwargs.update({
                    'batch': batch, 
                    'user_hist': user_hist,
                    'T': self.config['sampling_temperature']
                })

            else:
                kwargs['query'] = query

            if hasattr(self.sampler, "_update"):
                self.sampler._update()  #TODO: add frequency
            pos_prob, neg_id, neg_prob = self.sampler(**kwargs)

        else:
            raise TypeError("`sampler` only support Sampler type.")        
        return (pos_prob, neg_id, neg_prob), query


class BaseRanker(Recommender):

    def _init_model(self, train_data):
        super()._init_model(train_data)
        self.retriever = None


    def _get_item_encoder(self, train_data):
        return torch.nn.Embedding(train_data.num_item-1, self.emded_dim, padding_idx=0)


    @abc.abstractmethod
    def forward(self, batch):
        # calculate scores
        pass


    def build_index(self):
        raise NotImplementedError("build_index for ranker not implemented now.")


    def training_step(self, batch):
        y_h = self.forward(batch)
        loss = self.loss_fn(y_h, (batch[self.frating]>3).float())
        return loss

    def validation_step(self, batch, batch_idx):
        eval_metric = self.config['val_metrics']
        return self._test_step(batch, eval_metric)
    
    def test_step(self, batch, batch_idx):
        eval_metric = self.config['val_metrics']
        return self._test_step(batch, eval_metric)
    
    def _test_step(self, batch, eval_metric):
        pred = self.forward(batch)
        target = (batch[self.frating]>3).float()
        eval_metric = eval.get_pred_metrics(eval_metric)
        result = [f(pred, target.int()) for n, f in eval_metric]
        return result
    

class BaseQueryEncoder(torch.nn.Module):
    def __init__(self, config, fields: Union[List, Tuple], train_data, item_encoder=None) -> None:
        super().__init__()
        self.config = config
        self.fields = fields
        self.fuid = train_data.fuid
        self.model = torch.nn.Embedding(train_data.num_users, self.config['embed_dim'], padding_idx=0)
        if item_encoder is not None:
            self.item_encoder = item_encoder
    
    def forward(self, batch):
        args = self.get_query_feat(batch)
        return self.model(args[self.fuid])

    def get_query_feat(self, batch):
        return {f: batch[f] for f in self.fields}





class BaseRetriever(Recommender):
    def __init__(self, config: Dict):
        super(BaseRetriever, self).__init__(config)
        self.neg_count = config['negative_count']
        self.score_func = self._get_score_func()
        self.use_index = config['ann'] is not None and \
            (not config['item_bias'] or \
                (isinstance(self.score_func, scorer.InnerProductScorer) or \
                 isinstance(self.score_func, scorer.EuclideanScorer)))


    def _get_item_encoder(self, train_data):
        return torch.nn.Embedding(train_data.num_items, self.embed_dim, padding_idx=0)


    def _get_query_encoder(self, train_data):
        return BaseQueryEncoder(self.config, [train_data.fuid], train_data, self.item_encoder)
        # return torch.nn.Embedding(train_data.num_users, self.embed_dim, padding_idx=0)


    def _get_score_func(self):
        return scorer.InnerProductScorer()


    # def _get_user_feat(self, batch_data):
    #     if len(self.user_fields) == 1:
    #         return batch_data[self.fuid]
    #     else:
    #         return dict((field, value) for field, value in batch_data.items() if field in self.user_fields)


    def _get_item_vector(self):
        if self.item_encoder is None:
            assert hasattr(self, 'item_vector') and self.item_vector is not None, \
                'model without item_encoder should have item_vector.'
            return None
        elif len(self.item_fields) == 1:
            return self.item_encoder.weight[1:]
        else:
            # TODO: the batch_size should be configured
            output = [self.item_encoder(batch) 
                for batch in self.item_feat.loader(batch_size=1024)]
            output = torch.cat(output, dim=0)
            return output[1:]


    def _init_model(self, train_data, retriever=None):
        super()._init_model(train_data, retriever)
        self.fiid = train_data.fiid
        assert self.fiid in self.fields
        # self.item_embedding = torch.nn.Embedding(train_data.num_items, self.embed_dim, padding_idx=0)
        # self.user_embedding = torch.nn.Embedding(train_data.num_items, self.embed_dim, padding_idx=0)
        self.item_encoder = self._get_item_encoder(train_data)
        self.query_encoder = self._get_query_encoder(train_data)
        # if len(self.user_fields) > 0:
        #     self.fuid = train_data.fuid
        #     self.user_encoder = self._get_user_encoder(train_data)
        if len(self.item_fields) > 1:
            self.item_feat = train_data.item_feat


    def forward(self, batch_data, full_score):
        pos_items = self._get_item_feat(batch_data)

        if self.sampler is not None:
            if self.neg_count is None:
                raise ValueError("`negative_count` value is required when \
                    `sampler` is not none.")

            (pos_prob, neg_item_idx, neg_prob), query = self.sampling(
                batch_data, 
                neg = self.neg_count,
                excluding_hist = self.config['excluding_hist']
            )
            pos_score = self.score_func(query, self.item_encoder(pos_items))
            if batch_data[self.fiid].dim() > 1:
                pos_score[batch_data[self.fiid] == 0] = -float('inf') # padding
            neg_items = self._get_item_feat(neg_item_idx)
            neg_score = self.score_func(query, self.item_encoder(neg_items))
            return pos_score, pos_prob, neg_score, neg_prob
        else:
            query = self.query_encoder(batch_data)
            pos_score = self.score_func(query, self.item_encoder(pos_items))
            if batch_data[self.fiid].dim() > 1:
                pos_score[batch_data[self.fiid] == 0] = -float('inf') # padding
            if full_score:
                all_item_scores = self.score_func(query, self.item_vector)
                return pos_score, all_item_scores
            else:
                return (pos_score, )


    # def query_encoder(self, batch):
    #     if self.user_encoder is not None:
    #         return self.user_encoder(self._get_user_feat(batch))



    def build_index(self):
        raise NotImplementedError("build_index  for ranker not implemented.")


    def topk(self, query, k, user_h):
        # TODO: complete
        # if self.config['retriever'] is None:
        #     # topk on all items
        #     pass
        # elif self.config['retriever']=='uniform':
        #     # unform(num_items)
        #     pass
        # elif self.config['retriever']=='retriever':
        #     # retriever_query = self.retriever.query_encoder(batch)
        #     score, topk_items = self.retriever.topk(batch, k0, user_h)
        

        more = user_h.size(1) if user_h is not None else 0
        if self.use_index:
            if isinstance(self.score_func, scorer.CosineScorer):
                score, topk_items = self.ann_index.search(
                    torch.nn.functional.normalize(query, dim=1).numpy(), k + more)
            else:
                score, topk_items = self.ann_index.search(query.numpy(), k + more)
        else:
            score, topk_items = torch.topk(self.score_func(query, self.item_vector), k + more)
        topk_items += 1
        if user_h is not None:
            existing, _ = user_h.sort()
            idx_ = torch.searchsorted(existing, topk_items)
            idx_[idx_ == existing.size(1)] = existing.size(1) - 1
            score[torch.gather(existing, 1, idx_) == topk_items] = -float('inf')
            score1, idx = score.topk(k)
            return score1, torch.gather(topk_items, 1, idx)
        else:
            return score, topk_items


    def _update_item_vector(self):
        item_vector = self._get_item_vector()
        if item_vector is not None:
            self.register_buffer('item_vector', item_vector.detach().clone())

        if self.use_index:
            self.ann_index = self.build_ann_index()


    def _get_score_func(self):
        return scorer.InnerProductScorer()


    def training_step(self, batch):
        y_h = self.forward(batch, isinstance(self.loss_fn, loss_func.FullScoreLoss))
        loss_value = self.loss_fn(batch[self.frating], *y_h)
        return loss_value
    

    def validation_step(self, batch):
        eval_metric = self.config['val_metrics']
        cutoff = self.config['cutoff'][0] if isinstance(self.config['cutoff'], list) else self.config['cutoff']
        return self._test_step(batch, eval_metric, [cutoff])


    def test_step(self, batch):
        eval_metric = self.config['test_metrics']
        cutoffs = self.config['cutoff'] if isinstance(self.config['cutoff'], list) else [self.config['cutoff']]
        return self._test_step(batch, eval_metric, cutoffs)


    def _test_step(self, batch, metric, cutoffs):
        rank_m = eval.get_rank_metrics(metric)
        topk = self.config['topk']
        bs = batch[self.frating].size(0)
        assert len(rank_m)>0
        query = self.query_encoder(batch)
        score, topk_items = self.topk(query, topk, batch['user_hist'])
        if batch[self.fiid].dim() > 1:
            target, _ = batch[self.fiid].sort()
            idx_ = torch.searchsorted(target, topk_items)
            idx_[idx_ == target.size(1)] = target.size(1) - 1
            label = torch.gather(target, 1, idx_) == topk_items
            pos_rating = batch[self.frating]
        else:
            label = batch[self.fiid].view(-1, 1) == topk_items
            pos_rating = batch[self.frating].view(-1, 1)
        return [func(label, pos_rating, cutoff) for cutoff in cutoffs for name, func in rank_m], bs
    