[2024-12-25 15:13:05] INFO dataset is read from ../datasets/ml-100k.
[2024-12-25 15:13:05] INFO dataset is read from ../datasets/ml-100k.
[2024-12-25 15:13:05] INFO 
Dataset Info: 

==================================================
item information: 
field     item_id   
type      token     
##        1153      
==================================================
user information: 
field     user_id   
type      token     
##        944       
==================================================
interaction information: 
field     user_id   item_id   rating    timestamp 
type      token     token     float     float     
##        944       1153      -         -         
==================================================
Sparsity: 0.909832
==================================================
[2024-12-25 15:13:05] INFO 
Dataset Info: 

==================================================
item information: 
field     item_id   
type      token     
##        1153      
==================================================
user information: 
field     user_id   
type      token     
##        944       
==================================================
interaction information: 
field     user_id   item_id   rating    timestamp 
type      token     token     float     float     
##        944       1153      -         -         
==================================================
Sparsity: 0.909832
==================================================
[2024-12-25 15:13:05] INFO Global seed set to 2019
[2024-12-25 15:13:05] INFO 
Model Config: 

accelerator=gpu
activation=gelu
ann=None
batch_size=1024
cutoff=10
dataset_neg_count=None
dataset_sampler=None
dropout=0.2
early_stop_mode=max
early_stop_patience=20
embed_dim=64
epochs=100
eval_batch_size=128
excluding_hist=False
gpu=[2]
head_num=2
hidden_size=128
item_bias=False
layer_norm_eps=1e-12
layer_num=2
learner=adam
learning_rate=0.001
mask_ratio=0.2
negative_count=1
num_workers=0
pooling_type=mask
save_path=./saved/
scheduler=None
seed=2019
split_ratio=2
steal=False
test_metrics=['recall', 'precision', 'map', 'ndcg', 'mrr', 'hit']
test_repetitive=True
topk=100
train_repetitive=True
val_metrics=['recall', 'ndcg']
weight_decay=1e-05
[2024-12-25 15:13:05] INFO save_dir:/data1/home/zhihao/code/RecAudit/open-source/RecAudit/RecStudio-main
[2024-12-25 15:13:05] INFO BERT4Rec(
  (loss_fn): SoftmaxLoss()
  (score_func): InnerProductScorer()
  (item_encoder): Embedding(1154, 64, padding_idx=0)
  (query_encoder): BERT4RecQueryEncoder(
    (item_encoder): Embedding(1154, 64, padding_idx=0)
    (position_emb): Embedding(20, 64)
    (transformer_layer): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=128, bias=True)
          (dropout): Dropout(p=0.2, inplace=False)
          (linear2): Linear(in_features=128, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=128, bias=True)
          (dropout): Dropout(p=0.2, inplace=False)
          (linear2): Linear(in_features=128, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (training_pooling_layer): SeqPoolingLayer(pooling_type=mask, keepdim=False)
    (eval_pooling_layer): SeqPoolingLayer(pooling_type=last, keepdim=False)
  )
)
[2024-12-25 15:13:05] INFO Global seed set to 2019
[2024-12-25 15:13:05] INFO 
Model Config: 

accelerator=gpu
activation=gelu
ann=None
batch_size=1024
cutoff=10
dataset_neg_count=None
dataset_sampler=None
dropout=0.2
early_stop_mode=max
early_stop_patience=20
embed_dim=64
epochs=100
eval_batch_size=128
excluding_hist=False
gpu=[1]
head_num=2
hidden_size=128
item_bias=False
layer_norm_eps=1e-12
layer_num=2
learner=adam
learning_rate=0.001
mask_ratio=0.2
negative_count=1
num_workers=0
pooling_type=mask
save_path=./saved/
scheduler=None
seed=2019
split_ratio=2
steal=False
test_metrics=['recall', 'precision', 'map', 'ndcg', 'mrr', 'hit']
test_repetitive=True
topk=100
train_repetitive=True
val_metrics=['recall', 'ndcg']
weight_decay=1e-05
[2024-12-25 15:13:06] INFO save_dir:/data1/home/zhihao/code/RecAudit/open-source/RecAudit/RecStudio-main
[2024-12-25 15:13:06] INFO BERT4Rec(
  (loss_fn): SoftmaxLoss()
  (score_func): InnerProductScorer()
  (item_encoder): Embedding(1154, 64, padding_idx=0)
  (query_encoder): BERT4RecQueryEncoder(
    (item_encoder): Embedding(1154, 64, padding_idx=0)
    (position_emb): Embedding(20, 64)
    (transformer_layer): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=128, bias=True)
          (dropout): Dropout(p=0.2, inplace=False)
          (linear2): Linear(in_features=128, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=128, bias=True)
          (dropout): Dropout(p=0.2, inplace=False)
          (linear2): Linear(in_features=128, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (training_pooling_layer): SeqPoolingLayer(pooling_type=mask, keepdim=False)
    (eval_pooling_layer): SeqPoolingLayer(pooling_type=last, keepdim=False)
  )
)
[2024-12-25 15:13:08] INFO Training: Epoch=  0 [recall@10=0.0127 ndcg@10=0.0047 train_loss=7.1043]
[2024-12-25 15:13:08] INFO Train time: 0.33783s. Valid time: 0.38877s
[2024-12-25 15:13:08] INFO recall@10 improved. Best value: 0.0127
[2024-12-25 15:13:08] INFO Best model checkpoiny saved in ./saved/BERT4Rec-2024-12-25-15-13-04.ckpt.
[2024-12-25 15:13:08] INFO Training: Epoch=  0 [recall@10=0.0127 ndcg@10=0.0047 train_loss=7.1043]
[2024-12-25 15:13:08] INFO Train time: 0.36943s. Valid time: 0.42151s
[2024-12-25 15:13:08] INFO recall@10 improved. Best value: 0.0127
[2024-12-25 15:13:08] INFO Best model checkpoiny saved in ./saved/BERT4Rec-2024-12-25-15-13-04.ckpt.
[2024-12-25 15:13:10] INFO Global seed set to 2019
[2024-12-25 15:13:10] INFO 
Model Config: 

accelerator=gpu
activation=gelu
ann=None
batch_size=1024
cutoff=10
dataset_neg_count=None
dataset_sampler=None
dropout=0.2
early_stop_mode=max
early_stop_patience=20
embed_dim=64
epochs=100
eval_batch_size=128
excluding_hist=False
gpu=[2]
head_num=2
hidden_size=128
item_bias=False
layer_norm_eps=1e-12
layer_num=2
learner=adam
learning_rate=0.001
mask_ratio=0.2
negative_count=1
num_workers=0
pooling_type=mask
save_path=./saved/
scheduler=None
seed=2019
split_ratio=2
steal=False
test_metrics=['recall', 'precision', 'map', 'ndcg', 'mrr', 'hit']
test_repetitive=True
topk=100
train_repetitive=True
val_metrics=['recall', 'ndcg']
weight_decay=1e-05
[2024-12-25 15:13:10] INFO save_dir:/data1/home/zhihao/code/RecAudit/open-source/RecAudit/RecStudio-main
[2024-12-25 15:13:10] INFO BERT4Rec(
  (loss_fn): SoftmaxLoss()
  (score_func): InnerProductScorer()
  (item_encoder): Embedding(1154, 64, padding_idx=0)
  (query_encoder): BERT4RecQueryEncoder(
    (item_encoder): Embedding(1154, 64, padding_idx=0)
    (position_emb): Embedding(20, 64)
    (transformer_layer): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=128, bias=True)
          (dropout): Dropout(p=0.2, inplace=False)
          (linear2): Linear(in_features=128, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=128, bias=True)
          (dropout): Dropout(p=0.2, inplace=False)
          (linear2): Linear(in_features=128, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (training_pooling_layer): SeqPoolingLayer(pooling_type=mask, keepdim=False)
    (eval_pooling_layer): SeqPoolingLayer(pooling_type=last, keepdim=False)
  )
)
[2024-12-25 15:13:10] INFO Global seed set to 2019
[2024-12-25 15:13:10] INFO 
Model Config: 

accelerator=gpu
activation=gelu
ann=None
batch_size=1024
cutoff=10
dataset_neg_count=None
dataset_sampler=None
dropout=0.2
early_stop_mode=max
early_stop_patience=20
embed_dim=64
epochs=100
eval_batch_size=128
excluding_hist=False
gpu=[1]
head_num=2
hidden_size=128
item_bias=False
layer_norm_eps=1e-12
layer_num=2
learner=adam
learning_rate=0.001
mask_ratio=0.2
negative_count=1
num_workers=0
pooling_type=mask
save_path=./saved/
scheduler=None
seed=2019
split_ratio=2
steal=False
test_metrics=['recall', 'precision', 'map', 'ndcg', 'mrr', 'hit']
test_repetitive=True
topk=100
train_repetitive=True
val_metrics=['recall', 'ndcg']
weight_decay=1e-05
[2024-12-25 15:13:10] INFO save_dir:/data1/home/zhihao/code/RecAudit/open-source/RecAudit/RecStudio-main
[2024-12-25 15:13:10] INFO BERT4Rec(
  (loss_fn): SoftmaxLoss()
  (score_func): InnerProductScorer()
  (item_encoder): Embedding(1154, 64, padding_idx=0)
  (query_encoder): BERT4RecQueryEncoder(
    (item_encoder): Embedding(1154, 64, padding_idx=0)
    (position_emb): Embedding(20, 64)
    (transformer_layer): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=128, bias=True)
          (dropout): Dropout(p=0.2, inplace=False)
          (linear2): Linear(in_features=128, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=128, bias=True)
          (dropout): Dropout(p=0.2, inplace=False)
          (linear2): Linear(in_features=128, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (training_pooling_layer): SeqPoolingLayer(pooling_type=mask, keepdim=False)
    (eval_pooling_layer): SeqPoolingLayer(pooling_type=last, keepdim=False)
  )
)
[2024-12-25 15:13:10] INFO Training: Epoch=  0 [recall@10=0.0127 ndcg@10=0.0047 train_loss=7.1043]
[2024-12-25 15:13:10] INFO Train time: 0.03441s. Valid time: 0.08406s
[2024-12-25 15:13:10] INFO recall@10 improved. Best value: 0.0127
[2024-12-25 15:13:10] INFO Best model checkpoiny saved in ./saved/BERT4Rec-2024-12-25-15-13-04.ckpt.
[2024-12-25 15:13:10] INFO Training: Epoch=  0 [recall@10=0.0127 ndcg@10=0.0047 train_loss=7.1043]
[2024-12-25 15:13:10] INFO Train time: 0.03426s. Valid time: 0.09367s
[2024-12-25 15:13:10] INFO recall@10 improved. Best value: 0.0127
[2024-12-25 15:13:10] INFO Best model checkpoiny saved in ./saved/BERT4Rec-2024-12-25-15-13-04.ckpt.
