[2024-12-25 15:16:08] INFO dataset is read from ../datasets/ml-100k.
[2024-12-25 15:16:08] INFO dataset is read from ../datasets/ml-100k.
[2024-12-25 15:16:09] INFO Method: Yeom et al., AUROC: 0.5824779909935495, FPR@10%FNR: 0.7898089171974523.
[2024-12-25 15:16:09] INFO Method: Sablayrolles et al., AUROC: 0.7302527485902065, FPR@10%FNR: 0.5477707006369427.
[2024-12-25 15:16:09] INFO Method: Watson et al., AUROC: 0.6868027100490892, FPR@10%FNR: 0.6624203821656051.
[2024-12-25 15:16:09] INFO Method: Carlini et al., AUROC: 0.6864375836747941, FPR@10%FNR: 0.7707006369426752.
[2024-12-25 15:16:09] INFO Method: Zhou et al., AUROC: 0.5802263783520629, FPR@10%FNR: 0.7834394904458599.
[2024-12-25 15:16:09] INFO Method: RecAudit, AUROC: 0.9249868148809283, FPR@10%FNR: 0.22929936305732485.
[2024-12-25 15:16:09] INFO End of the code.
[2024-12-25 15:16:09] INFO ####################################################################################################
[2024-12-25 15:16:09] INFO 
Dataset Info: 

==================================================
item information: 
field     item_id   
type      token     
##        1153      
==================================================
user information: 
field     user_id   
type      token     
##        944       
==================================================
interaction information: 
field     user_id   item_id   rating    timestamp 
type      token     token     float     float     
##        944       1153      -         -         
==================================================
Sparsity: 0.909832
==================================================
[2024-12-25 15:16:09] INFO 
Dataset Info: 

==================================================
item information: 
field     item_id   
type      token     
##        1153      
==================================================
user information: 
field     user_id   
type      token     
##        944       
==================================================
interaction information: 
field     user_id   item_id   rating    timestamp 
type      token     token     float     float     
##        944       1153      -         -         
==================================================
Sparsity: 0.909832
==================================================
[2024-12-25 15:16:09] INFO Global seed set to 2019
[2024-12-25 15:16:09] INFO 
Model Config: 

accelerator=gpu
activation=gelu
ann=None
batch_size=1024
cutoff=10
dataset_neg_count=None
dataset_sampler=None
dropout=0.2
early_stop_mode=max
early_stop_patience=20
embed_dim=64
epochs=100
eval_batch_size=128
excluding_hist=False
gpu=[1]
head_num=2
hidden_size=128
item_bias=False
layer_norm_eps=1e-12
layer_num=2
learner=adam
learning_rate=0.001
mask_ratio=0.2
negative_count=1
num_workers=0
pooling_type=mask
save_path=./saved/
scheduler=None
seed=2019
split_ratio=2
steal=False
test_metrics=['recall', 'precision', 'map', 'ndcg', 'mrr', 'hit']
test_repetitive=True
topk=100
train_repetitive=True
val_metrics=['recall', 'ndcg']
weight_decay=1e-05
[2024-12-25 15:16:09] INFO save_dir:/data1/home/zhihao/code/RecAudit/open-source/RecAudit/RecStudio-main
[2024-12-25 15:16:09] INFO BERT4Rec(
  (loss_fn): SoftmaxLoss()
  (score_func): InnerProductScorer()
  (item_encoder): Embedding(1154, 64, padding_idx=0)
  (query_encoder): BERT4RecQueryEncoder(
    (item_encoder): Embedding(1154, 64, padding_idx=0)
    (position_emb): Embedding(20, 64)
    (transformer_layer): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=128, bias=True)
          (dropout): Dropout(p=0.2, inplace=False)
          (linear2): Linear(in_features=128, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=128, bias=True)
          (dropout): Dropout(p=0.2, inplace=False)
          (linear2): Linear(in_features=128, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (training_pooling_layer): SeqPoolingLayer(pooling_type=mask, keepdim=False)
    (eval_pooling_layer): SeqPoolingLayer(pooling_type=last, keepdim=False)
  )
)
[2024-12-25 15:16:09] INFO Global seed set to 2019
[2024-12-25 15:16:09] INFO 
Model Config: 

accelerator=gpu
activation=gelu
ann=None
batch_size=1024
cutoff=10
dataset_neg_count=None
dataset_sampler=None
dropout=0.2
early_stop_mode=max
early_stop_patience=20
embed_dim=64
epochs=100
eval_batch_size=128
excluding_hist=False
gpu=[2]
head_num=2
hidden_size=128
item_bias=False
layer_norm_eps=1e-12
layer_num=2
learner=adam
learning_rate=0.001
mask_ratio=0.2
negative_count=1
num_workers=0
pooling_type=mask
save_path=./saved/
scheduler=None
seed=2019
split_ratio=2
steal=False
test_metrics=['recall', 'precision', 'map', 'ndcg', 'mrr', 'hit']
test_repetitive=True
topk=100
train_repetitive=True
val_metrics=['recall', 'ndcg']
weight_decay=1e-05
[2024-12-25 15:16:09] INFO save_dir:/data1/home/zhihao/code/RecAudit/open-source/RecAudit/RecStudio-main
[2024-12-25 15:16:09] INFO BERT4Rec(
  (loss_fn): SoftmaxLoss()
  (score_func): InnerProductScorer()
  (item_encoder): Embedding(1154, 64, padding_idx=0)
  (query_encoder): BERT4RecQueryEncoder(
    (item_encoder): Embedding(1154, 64, padding_idx=0)
    (position_emb): Embedding(20, 64)
    (transformer_layer): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=128, bias=True)
          (dropout): Dropout(p=0.2, inplace=False)
          (linear2): Linear(in_features=128, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=128, bias=True)
          (dropout): Dropout(p=0.2, inplace=False)
          (linear2): Linear(in_features=128, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (training_pooling_layer): SeqPoolingLayer(pooling_type=mask, keepdim=False)
    (eval_pooling_layer): SeqPoolingLayer(pooling_type=last, keepdim=False)
  )
)
[2024-12-25 15:16:12] INFO Training: Epoch=  0 [recall@10=0.0127 ndcg@10=0.0047 train_loss=7.1043]
[2024-12-25 15:16:12] INFO Train time: 0.35803s. Valid time: 0.43081s
[2024-12-25 15:16:12] INFO recall@10 improved. Best value: 0.0127
[2024-12-25 15:16:12] INFO Best model checkpoiny saved in ./saved/BERT4Rec-2024-12-25-15-16-08.ckpt.
[2024-12-25 15:16:12] INFO Training: Epoch=  0 [recall@10=0.0127 ndcg@10=0.0047 train_loss=7.1043]
[2024-12-25 15:16:12] INFO Train time: 0.33922s. Valid time: 0.38942s
[2024-12-25 15:16:12] INFO recall@10 improved. Best value: 0.0127
[2024-12-25 15:16:12] INFO Best model checkpoiny saved in ./saved/BERT4Rec-2024-12-25-15-16-08.ckpt.
[2024-12-25 15:16:14] INFO Global seed set to 2019
[2024-12-25 15:16:14] INFO 
Model Config: 

accelerator=gpu
activation=gelu
ann=None
batch_size=1024
cutoff=10
dataset_neg_count=None
dataset_sampler=None
dropout=0.2
early_stop_mode=max
early_stop_patience=20
embed_dim=64
epochs=100
eval_batch_size=128
excluding_hist=False
gpu=[2]
head_num=2
hidden_size=128
item_bias=False
layer_norm_eps=1e-12
layer_num=2
learner=adam
learning_rate=0.001
mask_ratio=0.2
negative_count=1
num_workers=0
pooling_type=mask
save_path=./saved/
scheduler=None
seed=2019
split_ratio=2
steal=False
test_metrics=['recall', 'precision', 'map', 'ndcg', 'mrr', 'hit']
test_repetitive=True
topk=100
train_repetitive=True
val_metrics=['recall', 'ndcg']
weight_decay=1e-05
[2024-12-25 15:16:14] INFO save_dir:/data1/home/zhihao/code/RecAudit/open-source/RecAudit/RecStudio-main
[2024-12-25 15:16:14] INFO BERT4Rec(
  (loss_fn): SoftmaxLoss()
  (score_func): InnerProductScorer()
  (item_encoder): Embedding(1154, 64, padding_idx=0)
  (query_encoder): BERT4RecQueryEncoder(
    (item_encoder): Embedding(1154, 64, padding_idx=0)
    (position_emb): Embedding(20, 64)
    (transformer_layer): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=128, bias=True)
          (dropout): Dropout(p=0.2, inplace=False)
          (linear2): Linear(in_features=128, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=128, bias=True)
          (dropout): Dropout(p=0.2, inplace=False)
          (linear2): Linear(in_features=128, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (training_pooling_layer): SeqPoolingLayer(pooling_type=mask, keepdim=False)
    (eval_pooling_layer): SeqPoolingLayer(pooling_type=last, keepdim=False)
  )
)
[2024-12-25 15:16:14] INFO Training: Epoch=  0 [recall@10=0.0127 ndcg@10=0.0047 train_loss=7.1043]
[2024-12-25 15:16:14] INFO Train time: 0.03573s. Valid time: 0.08485s
[2024-12-25 15:16:14] INFO recall@10 improved. Best value: 0.0127
[2024-12-25 15:16:14] INFO Best model checkpoiny saved in ./saved/BERT4Rec-2024-12-25-15-16-08.ckpt.
[2024-12-25 15:16:15] INFO Global seed set to 2019
[2024-12-25 15:16:15] INFO 
Model Config: 

accelerator=gpu
activation=gelu
ann=None
batch_size=1024
cutoff=10
dataset_neg_count=None
dataset_sampler=None
dropout=0.2
early_stop_mode=max
early_stop_patience=20
embed_dim=64
epochs=100
eval_batch_size=128
excluding_hist=False
gpu=[1]
head_num=2
hidden_size=128
item_bias=False
layer_norm_eps=1e-12
layer_num=2
learner=adam
learning_rate=0.001
mask_ratio=0.2
negative_count=1
num_workers=0
pooling_type=mask
save_path=./saved/
scheduler=None
seed=2019
split_ratio=2
steal=False
test_metrics=['recall', 'precision', 'map', 'ndcg', 'mrr', 'hit']
test_repetitive=True
topk=100
train_repetitive=True
val_metrics=['recall', 'ndcg']
weight_decay=1e-05
[2024-12-25 15:16:15] INFO save_dir:/data1/home/zhihao/code/RecAudit/open-source/RecAudit/RecStudio-main
[2024-12-25 15:16:15] INFO BERT4Rec(
  (loss_fn): SoftmaxLoss()
  (score_func): InnerProductScorer()
  (item_encoder): Embedding(1154, 64, padding_idx=0)
  (query_encoder): BERT4RecQueryEncoder(
    (item_encoder): Embedding(1154, 64, padding_idx=0)
    (position_emb): Embedding(20, 64)
    (transformer_layer): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=128, bias=True)
          (dropout): Dropout(p=0.2, inplace=False)
          (linear2): Linear(in_features=128, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
          )
          (linear1): Linear(in_features=64, out_features=128, bias=True)
          (dropout): Dropout(p=0.2, inplace=False)
          (linear2): Linear(in_features=128, out_features=64, bias=True)
          (norm1): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (training_pooling_layer): SeqPoolingLayer(pooling_type=mask, keepdim=False)
    (eval_pooling_layer): SeqPoolingLayer(pooling_type=last, keepdim=False)
  )
)
[2024-12-25 15:16:15] INFO Training: Epoch=  0 [recall@10=0.0127 ndcg@10=0.0047 train_loss=7.1043]
[2024-12-25 15:16:15] INFO Train time: 0.04608s. Valid time: 0.11997s
[2024-12-25 15:16:15] INFO recall@10 improved. Best value: 0.0127
[2024-12-25 15:16:15] INFO Best model checkpoiny saved in ./saved/BERT4Rec-2024-12-25-15-16-08.ckpt.
